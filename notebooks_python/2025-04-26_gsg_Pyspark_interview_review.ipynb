{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d035b3-09a6-4250-a180-10ad293b6f8d",
   "metadata": {},
   "source": [
    "# PySpark Interview Review\n",
    "The goal of this notebook is to review basic Pyspark synthax for Data Engineering interview questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a09609-b33c-424e-a55b-f7d60a2f0f64",
   "metadata": {},
   "source": [
    "# Creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2aeb471-8286-4b3f-9c59-93029df3c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cebe90-01a6-4750-af93-d50d8bce40c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/26 16:14:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('PySparkReview').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45887710-4309-4b5e-b02c-b1e16edb07c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f73ef2-a5a1-4078-994d-7bd39a9838b9",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372cdc5-ff9f-4e4c-85dc-84e73e5d7a3d",
   "metadata": {},
   "source": [
    "## How to convert the index of a PySpark DataFrame into a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88c0d53-295a-4d3c-9625-8095da936856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  Alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b1c5a2-7c46-4857-a767-5fe52a42af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4857b1a1-23d6-4f0e-bb30-378e055c04ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    0|\n",
      "|    Bob|    2|    1|\n",
      "|Charlie|    3|    2|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 16:18:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/26 16:18:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/04/26 16:18:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('index', row_number().over(w) - 1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52154ff-b457-44ba-8de4-5c21db7fa3ce",
   "metadata": {},
   "source": [
    "## How to combine many lists to form a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39e8df1d-5eb1-4b35-93c1-e8222cdfb4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e68eb990-fde0-4cae-990a-614d346308a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3), ('d', 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(list1, list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f351655-e4e1-4ce8-b828-40c2262e2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(list(zip(list1, list2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e795bae-261d-4cff-a81a-6528786206cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|Col1|Col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   c|   3|\n",
      "|   d|   4|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF(['Col1', 'Col2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed5fec-9180-4650-807c-f0c5862f01ef",
   "metadata": {},
   "source": [
    "## How to get the items of list A not present in list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1dd63e9-ee60-4b43-9df3-c438e958694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a = [1, 2, 3, 4, 5]\n",
    "list_b = [4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93c52dfd-feca-4325-90fc-b581192b3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd_a = sc.parallelize(list_a)\n",
    "rdd_b = sc.parallelize(list_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1486fd9-0982-4534-8775-5fed7e2b95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_a_minus_b = rdd_a.subtract(rdd_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac779458-3d2b-4a92-b7d8-8602bb2dda51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_a_minus_b.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52393b79-9afe-4aa6-9975-34421377cbe2",
   "metadata": {},
   "source": [
    "## How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebd9d84d-da98-489a-b44a-985ba52b89c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60ab3d14-de8a-49ed-99e0-08c2fc41c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10b9cdbf-1c44-4ec0-8189-b18ef83814a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 20.0, 30.0, 50.0, 86.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5550f9b2-f5a6-4890-bb67-78fcc338c780",
   "metadata": {},
   "source": [
    "## How to get frequency counts of unique items of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac1b83f0-bdcb-49ff-9189-537943857be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e4a9d42-7e0e-42d8-b371-1037dc65f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "| Engineer|    4|\n",
      "|Scientist|    2|\n",
      "|   Doctor|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('job').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cab7d-1928-47d6-944d-a715f456bf52",
   "metadata": {},
   "source": [
    "## How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e6d3a63-5e80-48d9-89ea-f155cd26b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8a2076d-71fc-43f1-86eb-4282c9a4dae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Engineer', 'Scientist']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "top2jobs =df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
    "top2jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de442418-9204-4c5f-a668-c9cbb284d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('job', when(col('job').isin(top2jobs), col('job')).otherwise('Other'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a13b034-421b-4a13-8a15-4bbc9e01041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|    Other|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991eaeb7-0896-44b5-aa66-b2ab7101dea9",
   "metadata": {},
   "source": [
    "## How to Drop rows with NA values specific to a particular column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "555ab176-b23c-4184-a1ac-bbfe67d94425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95c750f9-eb2c-441a-ada7-35c1f75d1bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B|    3| 456|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.dropna(subset=['Value'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e70d0-8cf3-4347-97c3-ef22394bdac5",
   "metadata": {},
   "source": [
    "## How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93cfb753-dfb9-48be-89f5-7ef01bed2b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d80fb1c5-9f60-4f5d-a435-f97900540890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('col1', 'new_col1'), ('col2', 'new_col2'), ('col3', 'new_col3')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(old_names, new_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23acae70-14b6-4912-b065-71e77287fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rename_pair in list(zip(old_names, new_names)):\n",
    "    df = df.withColumnRenamed(rename_pair[0], rename_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb5e98d6-5a61-48e1-9a2f-1a971b553e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bade3ff-6dca-4332-a071-221ab785924c",
   "metadata": {},
   "source": [
    "## How to create contigency table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "233ab587-80ac-4122-bc36-03ea20478e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|category1|category2|\n",
      "+---------+---------+\n",
      "|        A|        X|\n",
      "|        A|        Y|\n",
      "|        A|        X|\n",
      "|        B|        Y|\n",
      "|        B|        X|\n",
      "|        C|        X|\n",
      "|        C|        X|\n",
      "|        C|        Y|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
    "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42643b22-a86e-455b-a495-200f085fb093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|category1|count|\n",
      "+---------+-----+\n",
      "|     NULL|    8|\n",
      "|        A|    3|\n",
      "|        B|    2|\n",
      "|        C|    3|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cube('category1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "819d9b2c-ec61-4631-b3ea-152984a7c50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+---+\n",
      "|category1_category2|  X|  Y|\n",
      "+-------------------+---+---+\n",
      "|                  B|  1|  1|\n",
      "|                  C|  2|  1|\n",
      "|                  A|  2|  1|\n",
      "+-------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crosstab('category1', 'category2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683aaf82-dfd4-4690-a564-43bed737c6f6",
   "metadata": {},
   "source": [
    "## How to stack two DataFrames vertically ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "95f98fae-7737-4a2d-b52a-c8701328bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_3|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea5b947d-2a73-41ed-bbd0-fc624213b71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_A.union(df_B).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09030a-7378-4ccb-89bc-16eb197b8d99",
   "metadata": {},
   "source": [
    "## How to convert the first character of each element in a series to uppercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2540b9b3-110b-4dc1-b780-2f6fa8dd5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "722bfd97-1eaf-4abc-b0b9-c292fc48adec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "daeded68-820d-4302-ba93-96b310d667ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('capitalized_name', initcap(col('name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2f7efd1-d219-4cab-b737-5413a975c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "| name|capitalized_name|\n",
      "+-----+----------------+\n",
      "| john|            John|\n",
      "|alice|           Alice|\n",
      "|  bob|             Bob|\n",
      "+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc1566-60a0-4dde-a741-691f6c0fbb3e",
   "metadata": {},
   "source": [
    "## How to compute summary statistics for all columns in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f067705-f1c0-41ef-90f3-de2f3132d5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "372eeb04-43cd-45a0-a59d-506ac6d90545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 16:46:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------------+-----------------+\n",
      "|summary|  name|              age|           salary|\n",
      "+-------+------+-----------------+-----------------+\n",
      "|  count|     5|                5|                5|\n",
      "|   mean|  NULL|             32.4|          66000.0|\n",
      "| stddev|  NULL|3.209361307176242|9617.692030835675|\n",
      "|    min| James|               29|            55000|\n",
      "|    25%|  NULL|               30|            60000|\n",
      "|    50%|  NULL|               32|            65000|\n",
      "|    75%|  NULL|               34|            70000|\n",
      "|    max|Robert|               37|            80000|\n",
      "+-------+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a410d-1cef-4dbc-bd3c-41331cc0e8bd",
   "metadata": {},
   "source": [
    "## How to calculate the number of characters in each word in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "647ce2fc-3b31-4581-97a9-8f5da25c6df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f3ce64ed-52c6-4baf-9af4-02ab0e126984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('word_len', length(col('name')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e48bb81-2fae-4c0e-b71c-9a0989a0289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| name|word_len|\n",
      "+-----+--------+\n",
      "| john|       4|\n",
      "|alice|       5|\n",
      "|  bob|       3|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2faa3ba-151f-4394-a1dc-d3449557fbb7",
   "metadata": {},
   "source": [
    "## How to get the day of month, week number, day of year and day of week from a date strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67a27326-c001-4b03-a343-099c5a531c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|date_str_1| date_str_2|\n",
      "+----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|\n",
      "|2023-12-31|01 Jan 2010|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1aac72e3-e7f5-4429-b4ec-5aed77e14299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "72904353-6bc3-40fb-a604-60d029b4d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('date_str_1', to_date(col('date_str_1'), 'yyyy-MM-dd'))\n",
    "df = df.withColumn('date_str_2', to_date(col('date_str_2'), 'dd MMM yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a415dc9d-7cf6-4125-a3df-6319ff0374c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('date_str_1', DateType(), True), StructField('date_str_2', DateType(), True)])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb971499-58f6-4562-a936-55e151105d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|date_str_1|date_str_2|\n",
      "+----------+----------+\n",
      "|2023-05-18|2010-01-01|\n",
      "|2023-12-31|2010-01-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce431872-7642-4b77-bb7a-f303d2d3bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    'day_of_month', dayofmonth(col('date_str_1'))\n",
    ").withColumn(\n",
    "    'week_numer', weekofyear(col('date_str_1'))\n",
    ").withColumn(\n",
    "    'day_of_year', dayofyear(col('date_str_1'))\n",
    ").withColumn(\n",
    "    'day_of_week', dayofweek(col('date_str_1'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22bc7e4f-efab-487e-a978-1228ff93ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+----------+-----------+-----------+\n",
      "|date_str_1|date_str_2|day_of_month|week_numer|day_of_year|day_of_week|\n",
      "+----------+----------+------------+----------+-----------+-----------+\n",
      "|2023-05-18|2010-01-01|          18|        20|        138|          5|\n",
      "|2023-12-31|2010-01-01|          31|        52|        365|          1|\n",
      "+----------+----------+------------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c083f61-e3eb-4106-b4e5-6782adfe4198",
   "metadata": {},
   "source": [
    "# Practicing on a real world Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "477749c0-5ab1-454a-a2f1-9ed2248d79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType, DoubleType, BooleanType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e74e4c1-9489-46f7-b55f-9b915db7ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('PySparkReview').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdbf2cad-e13c-4e83-8662-be0c4511514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"sex\", StringType(), True),          \n",
    "    StructField(\"dataset\", StringType(), True), \n",
    "    StructField(\"cp\", StringType(), True),       \n",
    "    StructField(\"trestbps\", IntegerType(), True),\n",
    "    StructField(\"chol\", IntegerType(), True),\n",
    "    StructField(\"fbs\", StringType(), True),         \n",
    "    StructField(\"restecg\", StringType(), True),     \n",
    "    StructField(\"thalch\", IntegerType(), True),\n",
    "    StructField(\"exang\", StringType(), True),   \n",
    "    StructField(\"oldpeak\", DoubleType(), True),\n",
    "    StructField(\"slope\", StringType(), True),    \n",
    "    StructField(\"ca\", IntegerType(), True),\n",
    "    StructField(\"thal\", StringType(), True),   \n",
    "    StructField(\"num\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f6e7b9d-c9cd-4da3-a42d-7cdc1cd636d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heartrate = spark.read.options(\n",
    "    header='True', delimiter=','\n",
    ").csv(\n",
    "    '/home/gabrielsgoncalves/Documents/Repositories/databricks_apache_spark_associate_certificate_study/datasets/heart_disease_uci.csv',\n",
    "    header=True,\n",
    "    schema=schema\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f717252-2942-40ef-8fbc-fe0a34bc98c5",
   "metadata": {},
   "source": [
    "## Getting an overview of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1cf145f-c679-438c-9cf2-ec3de84608e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------+-------------+--------------+------------------+\n",
      "|summary|              age|   sex|      dataset|            cp|          trestbps|\n",
      "+-------+-----------------+------+-------------+--------------+------------------+\n",
      "|  count|              920|   920|          920|           920|               861|\n",
      "|   mean|53.51086956521739|  NULL|         NULL|          NULL|132.13240418118468|\n",
      "| stddev|9.424685209576863|  NULL|         NULL|          NULL|19.066069518587465|\n",
      "|    min|               28|Female|    Cleveland|  asymptomatic|                 0|\n",
      "|    25%|               47|  NULL|         NULL|          NULL|               120|\n",
      "|    50%|               54|  NULL|         NULL|          NULL|               130|\n",
      "|    75%|               60|  NULL|         NULL|          NULL|               140|\n",
      "|    max|               77|  Male|VA Long Beach|typical angina|               200|\n",
      "+-------+-----------------+------+-------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_heartrate.select(['age', 'sex', 'dataset', 'cp', 'trestbps']).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3467c-f01b-4376-8c24-dc7ce7b92909",
   "metadata": {},
   "source": [
    "## Saving file as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14185c3d-0249-4893-bfde-9197c203b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heartrate.write.parquet('/home/gabrielsgoncalves/Documents/Repositories/databricks_apache_spark_associate_certificate_study/datasets/heart_disease_uci.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e389da7-9154-4d17-80f5-bbd7beca071f",
   "metadata": {},
   "source": [
    "## Reading the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a70e4758-b67f-4edd-b83a-d9ca02a65485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heartrate = spark.read.parquet(\n",
    "    '/home/gabrielsgoncalves/Documents/Repositories/databricks_apache_spark_associate_certificate_study/datasets/heart_disease_uci.parquet/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83cf9595-3f8a-46b9-a1fc-7e287fe47181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+---------+---------------+--------+----+-----+--------------+------+-----+-------+-----------+---+-----------------+---+\n",
      "| id|age|   sex|  dataset|             cp|trestbps|chol|  fbs|       restecg|thalch|exang|oldpeak|      slope| ca|             thal|num|\n",
      "+---+---+------+---------+---------------+--------+----+-----+--------------+------+-----+-------+-----------+---+-----------------+---+\n",
      "|  1| 63|  Male|Cleveland| typical angina|     145| 233| TRUE|lv hypertrophy|   150|FALSE|    2.3|downsloping|  0|     fixed defect|  0|\n",
      "|  2| 67|  Male|Cleveland|   asymptomatic|     160| 286|FALSE|lv hypertrophy|   108| TRUE|    1.5|       flat|  3|           normal|  2|\n",
      "|  3| 67|  Male|Cleveland|   asymptomatic|     120| 229|FALSE|lv hypertrophy|   129| TRUE|    2.6|       flat|  2|reversable defect|  1|\n",
      "|  4| 37|  Male|Cleveland|    non-anginal|     130| 250|FALSE|        normal|   187|FALSE|    3.5|downsloping|  0|           normal|  0|\n",
      "|  5| 41|Female|Cleveland|atypical angina|     130| 204|FALSE|lv hypertrophy|   172|FALSE|    1.4|  upsloping|  0|           normal|  0|\n",
      "+---+---+------+---------+---------------+--------+----+-----+--------------+------+-----+-------+-----------+---+-----------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_heartrate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe75cc84-0691-4746-adb5-799827841235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "920"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heartrate.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b5e6a-df59-419a-a086-b0c66ed587d6",
   "metadata": {},
   "source": [
    "## Get the average age for male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2e8e159-126e-40ba-b27b-4293a501c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7fd7bff-7624-41ea-bba3-a846e33af313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|   sex|avg_age|\n",
      "+------+-------+\n",
      "|Female|  52.47|\n",
      "|  Male|  53.79|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_heartrate.groupBy('sex').agg(round(avg('age'), 2).alias('avg_age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61319487-ca36-4e23-bae2-a1f4e86c661b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', StringType(), True), StructField('age', StringType(), True), StructField('sex', StringType(), True), StructField('dataset', StringType(), True), StructField('cp', StringType(), True), StructField('trestbps', StringType(), True), StructField('chol', StringType(), True), StructField('fbs', StringType(), True), StructField('restecg', StringType(), True), StructField('thalch', StringType(), True), StructField('exang', StringType(), True), StructField('oldpeak', StringType(), True), StructField('slope', StringType(), True), StructField('ca', StringType(), True), StructField('thal', StringType(), True), StructField('num', StringType(), True)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heartrate.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebd6ab-f39d-4da1-93eb-9682d7f334c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practicing the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
